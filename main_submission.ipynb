{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Data Filtering Pipeline for Text to Video Generation\n","Reducing training dataset size for *Text to Video Generation* by utilising static and dynamic video analysing techniques.\n","\n","\n","\n","Developed by: team SuperShy"]},{"cell_type":"markdown","metadata":{},"source":["## Preparation\n","In this section, we will perform the necessary preparations before starting the main analysis. This includes installing dependencies, and creating helper functions for later inference\n"]},{"cell_type":"markdown","metadata":{},"source":["### Install Dependencies\n","*source: requirements.txt*\n","\n","Use the requirements.txt file to download all the required dependencies"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-06T15:41:43.914633Z","iopub.status.busy":"2024-03-06T15:41:43.913909Z","iopub.status.idle":"2024-03-06T15:41:56.315958Z","shell.execute_reply":"2024-03-06T15:41:56.314535Z","shell.execute_reply.started":"2024-03-06T15:41:43.914602Z"},"trusted":true},"outputs":[],"source":["# the required libraries are available on the requirements.txt file\n","!pip install -r \"requirements.txt\" -q"]},{"cell_type":"markdown","metadata":{},"source":["### Helper Functions \n","*source: get_data_idx_range.py, dataset_generator.py*"]},{"cell_type":"markdown","metadata":{},"source":["#### Reading Datas\n","The given dataset is in the form of JSON. We will need to create some helper functions in order to correctly extract the data. Since the given data is very large, we will only sample some of the data from the dataset.\n","\n","Our code will sample the data by taking the video from index *start_idx* to *end_idx*."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-06T15:41:56.319221Z","iopub.status.busy":"2024-03-06T15:41:56.318839Z","iopub.status.idle":"2024-03-06T15:41:56.326898Z","shell.execute_reply":"2024-03-06T15:41:56.326025Z","shell.execute_reply.started":"2024-03-06T15:41:56.319187Z"},"trusted":true},"outputs":[],"source":["# file: get_data_idx_range.py\n","import json\n","\n","# get videos per batch\n","import json\n","\n","def get_data_idx_range(data, list_keys, start_idx, end_idx, save_to_json=False):\n","    keys = list_keys[start_idx:end_idx+1]\n","\n","    # take random n records from list_keys\n","    records = {}\n","    for key in keys:\n","        records[key] = data[key]\n","\n","    if save_to_json:\n","        # Write the random n records to a new JSON file\n","        with open(f'metafiles/hdvg_batch_{start_idx}-{end_idx}.json', 'w') as f:\n","            json.dump(records, f)\n","    \n","    return records"]},{"cell_type":"markdown","metadata":{},"source":["#### Creating Dataset Generator\n","\n","The video in our dataset are represented by its Youtube id. Hence, we will create a generator to help us download, split, and cut the videos asynchronusly by utilising multi-threading."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-06T15:42:37.259377Z","iopub.status.busy":"2024-03-06T15:42:37.259006Z","iopub.status.idle":"2024-03-06T15:42:37.306617Z","shell.execute_reply":"2024-03-06T15:42:37.305578Z","shell.execute_reply.started":"2024-03-06T15:42:37.259346Z"},"trusted":true},"outputs":[],"source":["# file: dataset_generator.py\n","import json\n","from pytube import YouTube\n","import cv2\n","import subprocess\n","import os\n","import shutil\n","import multiprocessing as mp\n","\n","class DatasetGenerator:\n","    def download_worker(self, q, vq, initialize_finished):\n","        while not q.empty() or not initialize_finished.is_set():\n","            try:\n","                video_id, video_info = q.get()\n","                self.download_video(video_id, video_info)\n","\n","                for clip_id, clip_info in video_info['clip'].items():\n","                    vq.put((video_id, clip_id, clip_info))\n","                print(\"Done downloading video: \" + video_id)\n","            except Exception as e:\n","                print(f\"Error when downloading video: {video_id}\")\n","            finally:\n","                q.task_done()\n","        \n","        return\n","\n","    def video_split_worker(self,q, sq, download_finished):\n","        while not q.empty() or not download_finished.is_set():\n","            try:\n","                video_id, clip_id, clip_info = q.get()\n","                self.split_video(clip_info, video_id, clip_id)\n","                for scene in clip_info['scene_split']:\n","                    sq.put((video_id, clip_id, scene))\n","                print(\"Done splitting video: \" + video_id)\n","            except Exception as e:\n","                print(f\"Error when splitting video: {video_id}\")\n","            finally:\n","                q.task_done()\n","        \n","        return\n","\n","\n","    def clip_split_worker(self,q, video_split_finished):\n","        while not q.empty() or not video_split_finished.is_set():\n","            try:\n","                video_id, clip_id, info_scene = q.get()\n","                self.split_clip(info_scene, self.data[video_id]['clip'][clip_id]['fps'], video_id, clip_id, info_scene[\"clip_id\"])\n","                print(\"Done splitting clip: \" + video_id + \"/\" + clip_id + \"/\" + info_scene[\"clip_id\"])\n","            except Exception as e:\n","                print(f\"Error when splitting clip: {video_id}/{clip_id}\")\n","            finally:\n","                q.task_done()\n","        \n","        return\n","\n","    def __init__(self, filename = None, generate_scene_video = True, generate_scene_samples = False, frame_output_folder = \"frames_output\", scenes_output_folder = \"video_clips\", download_output_folder=\"download_videos\", tmp_output_folder=\"tmp_clips\"):\n","        self.filename = filename\n","        self.generate_scene_video = generate_scene_video\n","        self.generate_scene_samples = generate_scene_samples\n","        self.frame_output_folder = frame_output_folder\n","        self.scenes_output_folder = scenes_output_folder\n","        self.download_output_folder = download_output_folder\n","        self.tmp_output_folder = tmp_output_folder\n","        self.data = None\n","        self.FRAME_INTERVAL = 250\n","\n","        os.makedirs(download_output_folder, exist_ok=True)\n","        os.makedirs(tmp_output_folder, exist_ok=True)\n","\n","        if self.generate_scene_video:\n","            os.makedirs(scenes_output_folder, exist_ok=True)\n","\n","        if self.generate_scene_samples:\n","            os.makedirs(self.frame_output_folder, exist_ok=True)\n","        \n","\n","    def cmd(self, cmd):\n","        proc = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)\n","        out, _ = proc.communicate()\n","        return out.decode('utf-8')\n","\n","    def hhmmss(self, timestamp1, timestamp2):\n","        hh,mm,s = timestamp1.split(':')\n","        ss,ms = s.split('.')\n","        timems1 = 3600*1000*int((hh)) +  60*1000*int(mm) + 1000*int(ss) + int(ms)\n","        hh,mm,s = timestamp2.split(':')\n","        ss,ms = s.split('.')\n","        timems2 = 3600*1000*int((hh)) +  60*1000*int(mm) + 1000*int(ss) + int(ms)\n","        dur = (timems2 - timems1)/1000\n","        return str(dur)\n","\n","    def download_video(self, video_id, video_info):\n","        # Checks if the video is already downloaded\n","        if os.path.exists(os.path.join(self.download_output_folder, f\"{video_id}.mp4\")):\n","            return\n","        \n","        url = video_info['url']\n","        file_name = f\"{video_id}.mp4\"\n","        \n","        yt = YouTube(url)\n","        stream = yt.streams.get_by_resolution('360p')\n","        stream.download(output_path=self.download_output_folder, filename=file_name)\n","\n","    def split_video(self, info, video_id, output_name):\n","        # info: a unit of a clip (span, scene_split, fps)\n","        # video_name = the name of the downloaded video\n","        # output_name = the name of the outputted video\n","        # cut hdvila clip\n","        yt_video = os.path.join(self.download_output_folder, video_id +'.mp4')\n","\n","        ori_clip_path = os.path.join(self.tmp_output_folder, output_name) # output the clip videos on temporary folder\n","        os.makedirs(os.path.join(self.tmp_output_folder, video_id), exist_ok=True)\n","        if not os.path.exists(ori_clip_path):\n","            sb = info['span']\n","            cmd = ['ffmpeg', '-ss', sb[0], '-t', self.hhmmss(sb[0], sb[1]),'-accurate_seek', '-i', yt_video, '-c', 'copy',\n","                '-avoid_negative_ts', '1', '-reset_timestamps', '1',\n","                '-y', '-hide_banner', '-loglevel', 'panic', '-map', '0', ori_clip_path]\n","            self.cmd(cmd)\n","\n","        if not os.path.isfile(ori_clip_path):\n","            raise Exception(f\"{ori_clip_path}: ffmpeg clip extraction failed\")\n","\n","    def split_clip(self, info_scene, fps, video_id, clip_input_name, scene_output_name):\n","        # info: a unit of a scene\n","        ori_clip_path = os.path.join(self.tmp_output_folder, clip_input_name)\n","\n","        clip_id = clip_input_name[:-4]\n","        \n","        try:\n","            start, end = int(info_scene['scene_cut'][0]), int(info_scene['scene_cut'][1])\n","            save_split_path = os.path.join(self.scenes_output_folder, video_id, scene_output_name + '.mp4')\n","\n","            # Skip if the scene is already split\n","            if os.path.exists(save_split_path):\n","                return\n","\n","            os.makedirs(os.path.join(self.scenes_output_folder, video_id), exist_ok=True)\n","            if end == -1:\n","                shutil.copy(ori_clip_path, save_split_path)\n","            else:\n","                oricap = cv2.VideoCapture(ori_clip_path)\n","                h = oricap.get(cv2.CAP_PROP_FRAME_HEIGHT)\n","                w = oricap.get(cv2.CAP_PROP_FRAME_WIDTH)\n","\n","                writer = cv2.VideoWriter(save_split_path, cv2.VideoWriter_fourcc(*\"mp4v\"), fps, (int(w),int(h)))\n","                oricap.set(cv2.CAP_PROP_POS_FRAMES, start+1)\n","                current = start+1\n","                frame_cnt = 0\n","                \n","                if self.generate_scene_samples:\n","                    os.makedirs(os.path.join(self.frame_output_folder, video_id, scene_output_name), exist_ok=True)\n","\n","                while current < end:\n","                    ret, frame = oricap.read()\n","\n","                    if self.generate_scene_samples and (frame_cnt * self.FRAME_INTERVAL < current / fps * 1000):\n","                        frame_name = os.path.join(self.frame_output_folder, video_id, scene_output_name, f\"{frame_cnt:04d}.jpg\")\n","                        cv2.imwrite(frame_name, frame)\n","                        frame_cnt += 1\n","                    \n","                    if self.generate_scene_video and ret:\n","                        writer.write(frame)\n","                    current += 1\n","                writer.release()\n","                oricap.release()\n","\n","        except Exception as e:\n","            print(\"Error occured\")\n","            print(e)\n","\n","    def load_data(self, data):\n","        self.data = data\n","        # print(\"Metadata loaded (data)\")\n","\n","    def load_from_file(self):\n","        with open(self.filename, 'r') as f:\n","            self.data = json.load(f)\n","        # print(\"Metadata loaded (from files)\")\n","    \n","    def download(self):\n","        for video_id, video_info in self.data.items():\n","            self.download_video(video_id, video_info)\n","        # print(\"Done downloading\")\n","\n","    def split_videos(self):\n","        for video_id, video_info in self.data.items():\n","            for clip_id, clip_info in video_info['clip'].items():\n","                self.split_video(clip_info, video_id, clip_id)\n","            # print(\"Done splitting videos: \" + video_id)\n","\n","            # Can delete video\n","        # print(\"Done splitting videos\")\n","    \n","    def split_scenes(self):\n","        for video_id, video_info in self.data.items():\n","            for clip_id, clip_info in video_info['clip'].items():\n","                for scene in clip_info['scene_split']:\n","                    self.split_clip(scene, clip_info['fps'], video_id, clip_id, scene[\"clip_id\"])\n","                    # print(\"Done splitting clips: \" + scene[\"clip_id\"])\n","        # print(\"Done splitting clips\")\n","\n","                \n","    \n","    def run(self):\n","        self.load_from_file()\n","        self.download()\n","        self.split_videos()\n","        self.split_scenes()\n","        print(\"Done\")\n","\n","        \n","    def run_threaded(self, num_of_downloader_threads = 2, num_of_video_splitter_threads = 2, num_of_clip_splitter_threads = 4):\n","\n","        if self.filename is not None:\n","            self.load_from_file()\n","\n","        dq = mp.JoinableQueue() # video id, video info\n","        vq = mp.JoinableQueue()\n","        sq = mp.JoinableQueue()\n","\n","        initialize_finished = mp.Event()\n","        download_finished = mp.Event()\n","        video_split_finished = mp.Event()\n","\n","        processes = []\n","\n","        for _ in range(num_of_downloader_threads):\n","            t = mp.Process(target=self.download_worker, args=(dq, vq, initialize_finished))\n","            t.start()\n","            processes.append(t)     \n","        print(\"Spawned download workers\")\n","\n","        for _ in range(num_of_video_splitter_threads):\n","            t = mp.Process(target=self.video_split_worker, args=(vq, sq, download_finished,))\n","            t.start()\n","            processes.append(t)\n","        print(\"Spawned video split workers\")\n","\n","        for _ in range(num_of_clip_splitter_threads):\n","            t = mp.Process(target=self.clip_split_worker, args=(sq, video_split_finished,))\n","            t.start()\n","            processes.append(t)\n","        print(\"Spawned clip split workers\")\n","\n","        for video_id, video_info in self.data.items():\n","            dq.put((video_id, video_info))\n","        \n","        initialize_finished.set()\n","        print(\"Initialization finished\")        \n","        dq.join()\n","        print(\"Download finished\")\n","        download_finished.set()\n","        vq.join()\n","        print(\"Video split finished\")\n","        video_split_finished.set()\n","        sq.join()\n","        print(\"Clip split finished\")\n","\n","        for p in processes:\n","            if p.is_alive():\n","                p.terminate()\n","\n","        return"]},{"cell_type":"markdown","metadata":{},"source":["### Creating Dataset \n","*source: dataset_class_batch.py*\n","\n","From the dataset, extract the important detail of each *scene split* and store it as a single data point\n","\n","Our code will utilise PyTorch framework to create the dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-06T15:41:56.394858Z","iopub.status.busy":"2024-03-06T15:41:56.394493Z","iopub.status.idle":"2024-03-06T15:41:56.409797Z","shell.execute_reply":"2024-03-06T15:41:56.408944Z","shell.execute_reply.started":"2024-03-06T15:41:56.394830Z"},"trusted":true},"outputs":[],"source":["# file: dataset_class_batch.py\n","from torch.utils.data import Dataset\n","import os\n","import json\n","\n","class VideoDataset(Dataset):\n","    def __init__(self, data, start_idx, end_idx):\n","        dg = DatasetGenerator(generate_scene_samples=True, generate_scene_video=True)\n","        self.list_keys = list(data.keys())\n","        self.data = get_data_idx_range(data, self.list_keys, \n","                                       start_idx, end_idx, \n","                                       save_to_json=False)\n","        self.data_list = list(self.data.values())\n","        dg.load_data(self.data)\n","        dg.run_threaded(num_of_clip_splitter_threads=6, \n","                        num_of_downloader_threads=2, \n","                        num_of_video_splitter_threads=2)\n","\n","        self.scene_data = []\n","        for video_id in self.data:\n","            for clip_id in self.data[video_id][\"clip\"]:\n","                for scene in self.data[video_id][\"clip\"][clip_id][\"scene_split\"]:\n","                    scene_dict = {}\n","                    scene_dict[\"scene_id\"] = scene[\"clip_id\"]\n","                    scene_dict[\"video_id\"] = video_id\n","                    scene_dict[\"clip_id\"] = clip_id[:-4]\n","                    scene_dict[\"caption\"] = scene[\"caption\"]\n","                    scene_dict[\"scene_cut\"] = scene[\"scene_cut\"]\n","                    scene_dict[\"video_path\"] = os.path.join('video_clips', video_id, scene_dict[\"scene_id\"] + '.mp4').replace('\\\\','/')\n","                    scene_dict[\"frames_path\"] = os.path.join('frames_output', video_id, scene[\"clip_id\"]).replace('\\\\','/')\n","\n","                    if os.path.exists(scene_dict[\"frames_path\"]) and os.path.exists(scene_dict[\"video_path\"]):\n","                        # ranges from 1s to 15s\n","                        if 4 <= len(os.listdir(scene_dict[\"frames_path\"])) <= 60:\n","                            self.scene_data.append(scene_dict)\n","    \n","    def __len__(self):\n","        return len(self.scene_data)\n","    \n","    def __getitem__(self, idx):\n","        # uncomment to print the scene data\n","        # print(idx, self.scene_data[idx])\n","        return self.scene_data[idx]"]},{"cell_type":"markdown","metadata":{},"source":["## Filtering\n","\n","In this section focuses on how to filter the dataset to only pick some *proposedly good video clips*. It is the main pipeline for this project, which we utilise several video analysing and classification techniques."]},{"cell_type":"markdown","metadata":{},"source":["### Generating Filtering Metrics \n","*source: static_check.py, optical_flow_check.py, image_to_embedding.py, inference.py*\n","\n","Not every data points can be considered good, and some might potentially hurt the model to be trained. Thus, it is essential to filter these datas and only choose some of the best. As a result, a more comprehensive and smaller dataset is obtained. \n","\n","The filtering process used in this notebook will be based on 2 features, *staticity of video* and *scene transitions*\n","\n","#### Staticity of Video\n","Staticity of the video is the metrics to score whether the main object of the video is moving or not (remains the same in its position). Static video would not be a great choice for the filtered dataset because it will cause the text-to-video model to not learn about object movement.\n","\n","#### Scene Transitions\n","Scene transitions determines whether there is a sudden change in the scene. This sudden change would hurt the performance of the model because the model will likely to loss the context of the previous frame. In another word, we want our model to always have the same context from the begining till the end. \n","\n","\n","We will discuss different approach to score these metrics"]},{"cell_type":"markdown","metadata":{},"source":["##### Energy-Based\n","\n","The difference of energy content can be calculated as the sum of squared difference of every pixel on each of the channel. We will calculate the energy content difference for each consecutive frames of the scene. A small energy content difference means that there is not much movement done by the main object. "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-06T15:41:56.411502Z","iopub.status.busy":"2024-03-06T15:41:56.410930Z","iopub.status.idle":"2024-03-06T15:41:56.425510Z","shell.execute_reply":"2024-03-06T15:41:56.424662Z","shell.execute_reply.started":"2024-03-06T15:41:56.411477Z"},"trusted":true},"outputs":[],"source":["# file: static_check.py\n","import numpy as np\n","import torch\n","\n","def get_static_difference(frames):\n","    diff = []\n","    for i in range(len(frames)-1):\n","        frame1 = frames[i] * 255\n","        frame2 = frames[i+1] * 255\n","        diff.append(torch.mean(torch.square(frame1 - frame2)))\n","    return np.max(diff), np.mean(diff)"]},{"cell_type":"markdown","metadata":{},"source":["##### Optical Flow\n","\n","Optical Flow is a method that can estimate the direction and magnitude of motion at each pixel or in regions of the image. By taking the sum of magnitudes, we can use optical flow as one of the key factors to determine the staticness of a video."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-06T15:41:56.427484Z","iopub.status.busy":"2024-03-06T15:41:56.427133Z","iopub.status.idle":"2024-03-06T15:41:56.440587Z","shell.execute_reply":"2024-03-06T15:41:56.439749Z","shell.execute_reply.started":"2024-03-06T15:41:56.427440Z"},"trusted":true},"outputs":[],"source":["# file: optical_flow_check.py\n","import cv2\n","import numpy as np\n","from PIL import Image\n","import os\n","from torchvision import transforms\n","\n","def image_transform(image):\n","    transform = transforms.Compose([\n","        transforms.Resize((320, 240)),\n","        transforms.CenterCrop(240),\n","        transforms.ToTensor(),  \n","    ])\n","    return transform(image).unsqueeze(0)\n","\n","def load_image(folder_path):\n","    # Sort the frames\n","    frames = sorted(os.listdir(folder_path))\n","    conv_frames = []\n","    \n","    # Convert the frames to tensor\n","    for frame in frames:\n","        image = Image.open(os.path.join(folder_path, frame)).convert('RGB')\n","        image = image_transform(image)\n","        conv_frames.append(image)\n","    return conv_frames\n","\n","def get_optical_flow(frames):\n","    avg_velocities = []\n","    prv = frames[0].squeeze(0).numpy().transpose((1, 2, 0)) * 255\n","    prv_gray = cv2.cvtColor(prv, cv2.COLOR_BGR2GRAY)\n","\n","    for i in range(len(frames)-1):\n","        nxt = frames[i+1].squeeze(0).numpy().transpose((1, 2, 0)) * 255\n","        nxt_gray = cv2.cvtColor(nxt, cv2.COLOR_BGR2GRAY)\n","\n","        flow = cv2.calcOpticalFlowFarneback(prv_gray, nxt_gray, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n","\n","        # Calculate the magnitude and angle of the 2D vectors\n","        magnitude, _ = cv2.cartToPolar(flow[..., 0], flow[..., 1])\n","\n","        # Calculate the average magnitude of the vectors, which corresponds to the average velocity\n","        average_velocity = np.mean(magnitude)\n","        avg_velocities.append(average_velocity)\n","\n","        # Update the previous frame\n","        prv = nxt\n","\n","    return np.max(avg_velocities), np.mean(avg_velocities)"]},{"cell_type":"markdown","metadata":{},"source":["##### Image Embedding\n","\n","Image embedding is one way how machine interprets a given image. A similar image embedding between 2 images means that the 2 images have similar context. Auto Encoder can be used to encode the image, while cosine similarity can be used to evaluate similarity of the embeddings."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-06T15:41:56.442756Z","iopub.status.busy":"2024-03-06T15:41:56.442397Z","iopub.status.idle":"2024-03-06T15:41:56.454248Z","shell.execute_reply":"2024-03-06T15:41:56.453424Z","shell.execute_reply.started":"2024-03-06T15:41:56.442727Z"},"trusted":true},"outputs":[],"source":["# file: image_to_embedding.py\n","import torch\n","import numpy as np\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","def tensor_to_flat_latent(tensor, model):\n","    with torch.inference_mode():\n","        y = model.encoder(tensor)\n","    return y.flatten()\n","\n","def MSELoss(x1, x2):\n","    loss = np.mean((x1 - x2) ** 2)\n","    return loss\n","\n","def cosine_similarity(x1, x2):\n","    norm_x1 = np.linalg.norm(x1)\n","    norm_x2 = np.linalg.norm(x2)\n","    cosine_score = np.dot(x1, x2) / (norm_x1 * norm_x2)\n","    return cosine_score\n","\n","def get_image_to_embedding(frames, model, device):\n","    final_mse = 0\n","    final_cos_sim = 0\n","\n","    curr_latent = tensor_to_flat_latent(frames[0].to(device), model).cpu().numpy()\n","    for i in range(len(frames) - 1):\n","        next_latent = tensor_to_flat_latent(frames[i+1].to(device), model).cpu().numpy()\n","        mse = MSELoss(curr_latent, next_latent)\n","        cos_sim = cosine_similarity(curr_latent, next_latent)\n","        final_mse += mse\n","        final_cos_sim += cos_sim\n","        curr_latent = next_latent\n","    \n","    final_mse = final_mse / (len(frames)-1)\n","    final_cos_sim = final_cos_sim / (len(frames)-1)\n","    return final_mse, final_cos_sim"]},{"cell_type":"markdown","metadata":{},"source":["##### Combined Filtering Metric Generation Process"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-06T15:41:56.456134Z","iopub.status.busy":"2024-03-06T15:41:56.455840Z","iopub.status.idle":"2024-03-06T15:41:56.481139Z","shell.execute_reply":"2024-03-06T15:41:56.480204Z","shell.execute_reply.started":"2024-03-06T15:41:56.456108Z"},"trusted":true},"outputs":[],"source":["#file: inference.py\n","\n","from diffusers import AutoencoderKL\n","import json\n","import os\n","from PIL import Image\n","import torch\n","from torchvision import transforms\n","import time\n","import pickle\n","import yaml\n","from tqdm import tqdm\n","\n","warnings.filterwarnings(\"ignore\")\n","os.environ['KMP_DUPLICATE_LIB_OK']='True'\n","def image_transform(image):\n","    transform = transforms.Compose([\n","        transforms.Resize((320, 240)),\n","        transforms.CenterCrop(240),\n","        transforms.ToTensor(),\n","    ])\n","    return transform(image).unsqueeze(0)\n","\n","\n","def load_image(folder_path):\n","    # Sort the frames\n","    frames = sorted(os.listdir(folder_path))\n","    conv_frames = []\n","\n","    # Convert the frames to tensor\n","    for frame in frames:\n","        image = Image.open(os.path.join(folder_path, frame)).convert('RGB')\n","        image = image_transform(image)\n","        conv_frames.append(image)\n","    return conv_frames\n","\n","\n","def get_model():\n","    model = AutoencoderKL.from_pretrained(\"stabilityai/sdxl-vae\")\n","    return model\n","\n","\n","def get_metrics(dataset, model, device):\n","    res = {}\n","    print(\"Processing the dataset metrics...\")\n","    print(\"Total number of scene videos: \", len(dataset))\n","    for idx, data in tqdm(enumerate(dataset)):\n","        scene_id = data['scene_id']\n","        frames_path = data['frames_path']\n","        frames = load_image(frames_path)\n","\n","        # Getting static difference\n","        max_rgb_diff, mean_rgb_diff = get_static_difference(frames)\n","\n","        # getting optical flow\n","        max_velocity, mean_velocity = get_optical_flow(frames)\n","\n","        # Getting image context similarity\n","        frame_mse, frame_cos_sim = get_image_to_embedding(frames, model, device)\n","\n","        # No. frames\n","        no_frames = len(frames)\n","\n","        # Storing the results\n","        res[scene_id] = {\n","            'max_rgb_diff': float(max_rgb_diff),\n","            'mean_rgb_diff': float(mean_rgb_diff),\n","            'max_velocity': float(max_velocity),\n","            'mean_velocity': float(mean_velocity),\n","            'mse': float(frame_mse),\n","            'cos_sim': float(frame_cos_sim),\n","            'no_frames': float(no_frames),\n","            'idx': idx,\n","            'clip_id': data['clip_id'],\n","        }"]},{"cell_type":"markdown","metadata":{},"source":["### Filtering Scenes\n","*source: filter_scenes.py*\n","\n","Function used to filter the scene based on a classification model"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#file: filter_scenes.py\n","import numpy\n","import pandas as pd\n","import json\n","import pickle\n","\n","def filter_scenes(data, n_taken, classifier_model):\n","    \"\"\"\n","    data consist of scene_id keys with dict containing: rgb_diff, avg_velocity, mse, cos_sim\n","    \"\"\"\n","\n","    # data is dict of dict and we want to convert it to list of dict\n","    data = list(data.values())\n","    data_ids = []\n","    clip_ids = []\n","    for scene_data in data:\n","        data_ids.append(scene_data['idx'])\n","        clip_ids.append(scene_data['clip_id'])\n","        scene_data.pop('idx')\n","        scene_data.pop('clip_id')\n","\n","    # convert to dataframe\n","    data = pd.DataFrame(data)\n","\n","    # get the prediction\n","    pred = classifier_model.predict_proba(data)[:, 1]\n","\n","    # sort data_ids and clip_ids based on pred descending\n","    sorted_data_ids = numpy.argsort(pred)[::-1]\n","    sorted_clip_ids = [clip_ids[i] for i in sorted_data_ids]\n","    sorted_data_ids = [data_ids[i] for i in sorted_data_ids]\n","\n","    # take the top n_taken but skip clips that have been taken\n","    filtered_data = []\n","    clips_taken = []\n","    for i in range(len(data)):\n","        if len(filtered_data) == n_taken:\n","            break\n","        if sorted_clip_ids[i] not in clips_taken:\n","            filtered_data.append(sorted_data_ids[i])\n","            clips_taken.append(sorted_clip_ids[i])\n","\n","    \n","    return filtered_data"]},{"cell_type":"markdown","metadata":{},"source":["### Building XGBoost Model for Classification\n","*source: human_evaluation.ipynb, building_xg_boost.ipynb*\n","\n","XGBoost is one powerful model to do classification task based on numeric features."]},{"cell_type":"markdown","metadata":{},"source":["#### Creating Dataset for Training"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# file: human_evaluation.ipynb\n","import cv2\n","from IPython.display import display\n","import random\n","\n","def display_video(video_path):\n","    # Display the video in a separate window popup\n","    cap = cv2.VideoCapture(video_path)\n","    while cap.isOpened():\n","        ret, frame = cap.read()\n","        if not ret:\n","            break\n","\n","        # Display the frame in a separate window\n","        cv2.imshow('Video', frame)\n","\n","        if cv2.waitKey(1) & 0xFF == ord('q'):\n","            break\n","\n","    cap.release()\n","    cv2.destroyAllWindows()\n","\n","def annotate_videos(dataset, annotation_dict):\n","    labels = []\n","    print(len(dataset))\n","\n","    no_clips = min(len(dataset), 3)\n","    ids = random.sample(range(len(dataset)), no_clips)\n","    \n","    for id in ids:\n","        video_sample = dataset[id]\n","        video_path = video_sample[\"video_path\"]\n","        frames_path = video_sample[\"frames_path\"]\n","        print(f\"video: {frames_path}\")\n","        \n","        # Display the video in a separate window popup\n","        display_video(video_path)\n","        \n","        # Get user input for label\n","        label = input(\"Enter the label for this video clip (0 or 1) (or 'q' to quit): \")\n","        while label != '0' and label != '1' and label != 'q':\n","            print(\"Invalid label. Please enter 0 or 1.\")\n","            display_video(video_path)\n","            label = input(\"Enter the label for this video clip (0 or 1) (or 'q' to quit): \")\n","        \n","        # Save the label and video file name to dataset\n","        if label != 'q':\n","            annotation_dict[frames_path] = int(label)\n","            labels.append(label)\n","        else:\n","            break\n","    \n","    return labels, annotation_dict"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# file: human_evaluation.ipynb\n","import json\n","\n","with open(\"metafiles/hdvg_0.json\", 'r') as f:\n","    data = json.load(f)\n","print(\"Data loaded\")\n","\n","total = 0\n","annotation_dict = {}\n","for idx in range(0,101,1):\n","    dataset = VideoDataset(data, idx, idx)\n","    labels, annotation_dict = annotate_videos(dataset, annotation_dict)\n","    total += len(labels)\n","    print(labels)\n","\n","print(annotation_dict)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# file: human_evaluation.ipynb\n","# Save the annotation dictionary to a json\n","import json\n","with open('annotations.json', 'w') as f:\n","    json.dump(annotation_dict, f)"]},{"cell_type":"markdown","metadata":{},"source":["#### Building The Model"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# file: building_xg_boost_model.ipynb\n","import xgboost as xgb\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# file: building_xg_boost_model.ipynb\n","from diffusers import AutoencoderKL\n","import torch\n","\n","import cv2\n","import numpy as np\n","from PIL import Image\n","import os\n","from torchvision import transforms\n","\n","\n","def image_transform(image):\n","    transform = transforms.Compose([\n","        transforms.Resize((320, 240)),\n","        transforms.CenterCrop(240),\n","        transforms.ToTensor(),  \n","    ])\n","    return transform(image).unsqueeze(0)\n","\n","def load_image(folder_path):\n","    # Sort the frames\n","    frames = sorted(os.listdir(folder_path))\n","    conv_frames = []\n","    \n","    # Convert the frames to tensor\n","    for frame in frames:\n","        image = Image.open(os.path.join(folder_path, frame)).convert('RGB')\n","        image = image_transform(image)\n","        conv_frames.append(image)\n","    return conv_frames\n","\n","def get_model():\n","    model = AutoencoderKL.from_pretrained(\"stabilityai/sdxl-vae\")\n","    return model\n","\n","def get_inference(folder_path, model, device):\n","    frames = load_image(folder_path)\n","\n","    # Getting static difference\n","    rgb_diff_max, rgb_diff_mean = get_static_difference(frames)\n","\n","    # getting optical flow\n","    max_velocity, avg_velocity = get_optical_flow(frames)\n","    \n","    # Getting image context similarity\n","    frame_mse, frame_cos_sim = get_image_to_embedding(frames, model, device)\n","\n","    # Getting number of frames\n","    num_frames = len(frames)\n","\n","    return rgb_diff_max, rgb_diff_mean, max_velocity, avg_velocity, frame_mse, frame_cos_sim, num_frames"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# file: building_xg_boost_model.ipynb\n","# Load from annotations.json\n","import json\n","with open('annotations.json', 'r') as f:\n","    labeled_data = json.load(f)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# file: building_xg_boost_model.ipynb\n","rgb_diff_maxs = []\n","rgb_diff_means = []\n","max_velocities = []\n","avg_velocities = []\n","frame_mses = []\n","frame_cos_sims = []\n","num_frames_list = []\n","labels = []"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# file: building_xg_boost_model.ipynb\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","model = get_model().to(device)\n","\n","for i, path in enumerate(labeled_data.keys()):\n","    rgb_diff_max, rgb_diff_mean, max_velocity, avg_velocity, frame_mse, frame_cos_sim, num_frames = get_inference(path, model, device)\n","    rgb_diff_maxs.append(rgb_diff_max)\n","    rgb_diff_means.append(rgb_diff_mean)\n","    max_velocities.append(max_velocity)\n","    avg_velocities.append(avg_velocity)\n","    frame_mses.append(frame_mse)\n","    frame_cos_sims.append(frame_cos_sim)\n","    num_frames_list.append(num_frames)\n","    labels.append(labeled_data[path])\n","    print(f'Enumerated {i+1} paths')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# file: building_xg_boost_model.ipynb\n","import pandas as pd\n","\n","dataset = pd.DataFrame({\n","    \"max_rgb_diff\": rgb_diff_maxs,\n","    \"mean_rgb_diff\": rgb_diff_means,\n","    \"max_velocity\": max_velocities,\n","    \"mean_velocity\": avg_velocities,\n","    \"mse\": frame_mses,\n","    \"cos_sim\": frame_cos_sims,\n","    \"no_frames\": num_frames_list,\n","    \"label\": labels\n","})"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# file: building_xg_boost_model.ipynb\n","X = dataset.drop(columns=[\"label\"])\n","y = dataset[\"label\"]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# file: building_xg_boost_model.ipynb\n","# Splitting the data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n","\n","# Creating an XGBoost classifier\n","model = xgb.XGBClassifier()\n","\n","# Training the model\n","eval_set = [(X_test, y_test)]\n","model.fit(X_train, y_train, early_stopping_rounds=10, eval_metric=\"logloss\", eval_set=eval_set, verbose=False)\n","\n","y_pred_train = model.predict(X_train)\n","accuracy_train = accuracy_score(y_train, y_pred_train)\n","print(\"Train Accuracy:\", accuracy_train)\n","\n","# Making predictions on the test set\n","y_pred = model.predict(X_test)\n","\n","# Evaluating the model\n","accuracy = accuracy_score(y_test, y_pred)\n","print(\"Accuracy:\", accuracy)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# file: building_xg_boost_model.ipynb\n","import pickle\n","\n","# Save the trained model to a file\n","filename = 'xgboost_model.pkl'\n","pickle.dump(model, open(filename, 'wb'))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# file: building_xg_boost_model.ipynb\n","# TO TEST LOADING THE SAVED MODEL\n","\n","# Load the saved model from file\n","classifier_model = pickle.load(open(filename, 'rb'))\n","\n","# Use the loaded model for predictions\n","y_pred_loaded = classifier_model.predict(X_test)\n","\n","# Evaluate the loaded model\n","accuracy_loaded = accuracy_score(y_test, y_pred_loaded)\n","print(\"Accuracy of loaded model:\", accuracy_loaded)"]},{"cell_type":"markdown","metadata":{},"source":["## Re-captioning the Chosen Dataset\n","\n","In this section, the obtained dataset will be improved accordingly by the help of the already available Generative AI"]},{"cell_type":"markdown","metadata":{},"source":["### Gemini Recaptioning Class\n","*source: gemini_recaptioning.py*\n","\n","Re-captioning is essential to provide more details of the generated video to the dataset."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# file: gemini_recaptioning.py\n","from IPython.display import Markdown\n","import google.generativeai as genai\n","\n","import json\n","import textwrap\n","import PIL.Image\n","import os\n","\n","from dotenv import load_dotenv\n","load_dotenv()\n","\n","def to_markdown(text):\n","  text = text.replace('•', '  *')\n","  return Markdown(textwrap.indent(text, '> ', predicate=lambda _: True))\n","\n","class GeminiRecaptioning:\n","    def __init__(self, api_key, data):\n","        self.safety_settings = [\n","            {\n","                \"category\": \"HARM_CATEGORY_DANGEROUS\",\n","                \"threshold\": \"BLOCK_NONE\",\n","            },\n","            {\n","                \"category\": \"HARM_CATEGORY_HARASSMENT\",\n","                \"threshold\": \"BLOCK_NONE\",\n","            },\n","            {\n","                \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n","                \"threshold\": \"BLOCK_NONE\",\n","            },\n","            {\n","                \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n","                \"threshold\": \"BLOCK_NONE\",\n","            },\n","            {\n","                \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n","                \"threshold\": \"BLOCK_NONE\",\n","            },\n","        ]\n","        self.api_key = api_key\n","        self.data = data\n","\n","        genai.configure(api_key = api_key)\n","        self.model = genai.GenerativeModel('gemini-pro-vision')\n","\n","        self.RESOLUTION = (240, 240)\n","\n","\n","    def __generate_prompt(self, images, caption):\n","        res = [f\"The original caption to be improved is: {caption}.\", \n","               \"Provide additional visual details from these video frames with these following rules:\",\n","               \"All lowercase\",\n","               \"No more than 100 words\",\n","               \"Only use information available in the image and no assumptions\",\n","               \"Answer in English only\",]\n","        for image in images:\n","            res.append(image)\n","        \n","        return res\n","     \n","    def run(self, frames_folder_path: str) -> str:\n","        scene_name = frames_folder_path.split('/')[-1]\n","        video_id = '.'.join(scene_name.split('.')[0:-1])\n","        clip_id = '_'.join(scene_name.split('_')[0:-1])\n","        scene_no =  int(scene_name.split('_')[-1])\n","\n","        scene_info = list(filter(lambda x: x['clip_id'] == scene_name, self.data[video_id]['clip'][clip_id+\".mp4\"]['scene_split']))[0]\n","\n","        folder_path = f'./frames_output/{video_id}/{scene_name}'\n","        count = len(os.listdir(folder_path))\n","        indexes = []\n","\n","        if count >= 3:\n","            indexes = [0, count // 2, count - 1]\n","        elif count == 2:\n","            indexes = [0, count - 1]\n","        else:\n","            indexes = [0]\n","\n","        \n","\n","        images =  []\n","\n","        for i in indexes:\n","            image_path = f'{folder_path}/{i:0>4}.jpg'\n","            image = PIL.Image.open(image_path).resize(self.RESOLUTION)\n","            images.append(image)\n","\n","        caption = scene_info['caption']\n","        content = self.__generate_prompt(images, caption)\n","\n","        for image in images:\n","            content.append(image)\n","\n","        response = self.model.generate_content(content, safety_settings=self.safety_settings)\n","        return response.text"]},{"cell_type":"markdown","metadata":{},"source":["## Evaluation\n","\n","In this section, we will provide the evidence to proof our chosen dataset works better than randomly choosing another dataset of the same length"]},{"cell_type":"markdown","metadata":{},"source":["### Evaluation Function\n","*source: evaluation.py*\n","\n","Evaluation is necessary to compare whether our reduced dataset can score better compared to the base dataset. Since our reduced dataset is smaller, then the base model must also have the same length, which will be chosen through random sampling. There are 2 features that we suggest is crucial for comparisson, video clips and captions. Our whole evaluation process are done through 2 different pipelines: video quality checking and caption quality checking."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# file: evaluation.py\n","import open_clip\n","import torch\n","import PIL\n","import json\n","import os\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","import logging\n","logging.basicConfig(filename = \"eval.log\", level = logging.INFO)\n","\n","def get_eval_dataset(path):\n","    with open(path, 'r') as file:\n","        eval_dataset = json.load(file)\n","        eval_dataset = eval_dataset['scenes']\n","\n","    return eval_dataset\n","\n","\n","def get_eval_model():\n","    # Load the model and tokenizer\n","    model, preprocess = open_clip.create_model_from_pretrained('hf-hub:laion/CLIP-ViT-g-14-laion2B-s12B-b42K')\n","    tokenizer = open_clip.get_tokenizer('hf-hub:laion/CLIP-ViT-g-14-laion2B-s12B-b42K')\n","\n","    return model, preprocess, tokenizer"]},{"cell_type":"markdown","metadata":{},"source":["#### Video Quality Checking\n","\n","To compare the quality of video, it is essential to keep the other variable (the caption) modified the same way. In our case, both the captions are re-generated, and thus, we are comparing *Re-captioned Reduced Dataset* vs *Re-captioned Randomly Sampled Dataset*. "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# file: evaluation.py\n","def eval_different_dataset(eval_dataset1, eval_dataset2, preprocess, model, tokenizer, device, logging):\n","    # This evaluation pipeline is used to evaluate the video quality\n","    cosine = torch.nn.functional.cosine_similarity\n","    print('Evaluating caption and image similarity on sampled original and refined datasets')\n","    total_score1 = 0\n","    total_score2 = 0\n","\n","    # Evaluate the similarity between the caption and the image of randomized and refined datasets\n","    # Both of these datasets have been recaptioned\n","    for i, data in enumerate(zip(eval_dataset1, eval_dataset2)):\n","        data1, data2 = data\n","        # Load the image, caption and recaption\n","        frames_path1, caption1 = data1['frames_path'], data1['recaption']\n","        frames_path2, caption2 = data2['frames_path'], data2['recaption']\n","        \n","        text_input1 = tokenizer([caption1])\n","        text_input2 = tokenizer([caption2])\n","        text_input1 = text_input1.to(device)\n","        text_input2 = text_input2.to(device)\n","\n","        frame_list1 = os.listdir(frames_path1)\n","        frame_list2 = os.listdir(frames_path2)\n","\n","        with torch.inference_mode():\n","            text_features1 = model.encode_text(text_input1)\n","            text_features2 = model.encode_text(text_input2)\n","\n","        sim1, sim2 = 0, 0\n","\n","        length = min(len(frame_list1), len(frame_list2))\n","        print(f'length: {length}')\n","\n","        for i in range(0, length, 2):\n","            endpath1 = frame_list1[i]\n","            endpath2 = frame_list2[i]\n","            print(f'{endpath1} | {endpath2}')\n","            imagepath1 = frames_path1 + '/' + endpath1\n","            imagepath2 = frames_path2 + '/' + endpath2\n","\n","            image1 = PIL.Image.open(imagepath1)\n","            image2 = PIL.Image.open(imagepath2)\n","\n","            image_input1 = preprocess(image1).unsqueeze(0)\n","            image_input2 = preprocess(image2).unsqueeze(0)\n","\n","            image_input1 = image_input1.to(device)\n","            image_input2 = image_input2.to(device)\n","\n","            with torch.inference_mode():\n","                image_features1 = model.encode_image(image_input1)\n","                image_features2 = model.encode_image(image_input2)\n","        \n","            # Calculate the similarity between the image and the caption of these two datasets\n","            sim1 += max(100 * cosine(image_features1, text_features1), 0)\n","            sim2 += max(100 * cosine(image_features2, text_features2), 0)\n","        \n","        sim1 = sim1.item() / length\n","        sim2 = sim2.item() / length\n","\n","        # Update the total score of each dataset\n","        total_score1 += sim1\n","        total_score2 += sim2 \n","        print(f'Random Dataset similarity: {total_score1/(i+1)} | Filtered Dataset similarity: {total_score2/(i+1)}')\n","        logging.info(f'Random Dataset similarity: {total_score1/(i+1)} | Filtered Dataset similarity: {total_score2/(i+1)}')\n","\n","    total_score1 = total_score1 / len(eval_dataset1)\n","    total_score2 = total_score2 / len(eval_dataset2)\n","    \n","    return total_score1, total_score2 "]},{"cell_type":"markdown","metadata":{},"source":["#### Caption Quality Checking\n","Similarly, to compare the quality of caption, is it essential to keep the videos modified the same way. Since there is no processing of videos, then we are comparing *Re-captioned Reduced Dataset* vs *Original Reduced Dataset*."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def eval_same_dataset(eval_dataset, preprocess, model, tokenizer, device, logging):\n","    # This evaluation pipeline is used to evaluate the caption quality\n","    cosine = torch.nn.functional.cosine_similarity\n","    print('Evaluating caption and recaption similarity on the same dataset')\n","    total_caption_score = 0\n","    total_recaption_score = 0\n","\n","    for i, data in enumerate(eval_dataset):\n","        # Load the image, caption and recaption\n","        frames_path, caption, recaption = data['frames_path'], data['caption'], data['recaption']\n","\n","        caption_input = tokenizer([caption])\n","        recaption_input = tokenizer([recaption])\n","        caption_input = caption_input.to(device)\n","        recaption_input = recaption_input.to(device)\n","    \n","        caption_image_sim = 0\n","        recaption_image_sim = 0\n","\n","        with torch.inference_mode():\n","            caption_features = model.encode_text(caption_input)\n","            recaption_features = model.encode_text(recaption_input)\n","\n","        frame_list = os.listdir(frames_path)\n","        for i in range(0, len(frame_list), 2):\n","            endpath = frame_list[i]\n","            imagepath = frames_path + '/' + endpath\n","            print(endpath)\n","            image = PIL.Image.open(imagepath)\n","            image_input = preprocess(image).unsqueeze(0)\n","            image_input = image_input.to(device)\n","            with torch.inference_mode():\n","                image_features = model.encode_image(image_input)\n","\n","            # Calculate the similarity between the image and the caption\n","            caption_image_sim += max(100 * cosine(image_features, caption_features), 0)\n","            recaption_image_sim += max(100 * cosine(image_features, recaption_features), 0) \n","\n","        caption_score = caption_image_sim.item() / len(frame_list)\n","        recaption_score = recaption_image_sim.item() / len(frame_list)\n","        \n","        # Update the total score of each dataset\n","        total_caption_score += caption_score\n","        total_recaption_score += recaption_score\n","        print(f'image {i+1} | caption: {total_caption_score/(i+1)} | recaption: {total_recaption_score/(i+1)}')\n","        logging.info(f'image {i+1} | caption: {total_caption_score/(i+1)} | recaption: {total_recaption_score/(i+1)}')\n","    \n","    total_caption_score = total_caption_score / len(eval_dataset)\n","    total_recaption_score = total_recaption_score / len(eval_dataset)\n","    return total_caption_score, total_recaption_score\n"]},{"cell_type":"markdown","metadata":{},"source":["## Inference\n","\n","In this section, the whole process would be executed to obtain the result needed based on the given dataset"]},{"cell_type":"markdown","metadata":{},"source":["### Inference Process\n","*source: inference.py*"]},{"cell_type":"markdown","metadata":{},"source":["We will follow this pipeline in order to filter the dataset:\n","1. Preparing the PyTorch Dataset (sample only each 100 videos due to time and memory constraint). This include downloading videos, splitting and cutting clips.\n","2. Calculate the filtering metrics, i.e. Staticity and Motion of Video\n","3. Use XGBoost to classify good/bad video, with filtering metrics as the features\n","4. Recaption the chosen video clips\n","5. Evaluate whether the chosen video is better than random picking\n","6. Evaluate whether re-captioning is better than not recaptioning"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","import logging\n","logging.basicConfig(filename = \"eval.log\", level = logging.INFO)\n","\n","from dotenv import load_dotenv\n","load_dotenv()\n","os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\n","\n","def select_random_scenes(dataset, n_taken):\n","    ids = torch.arange(len(dataset))\n","    select_ids = ids[torch.randperm(len(dataset))[:n_taken]]\n","    return select_ids"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["N_VIDEOS_PER_BATCH = 4\n","N_TOTAL_VIDEOS = 18_750\n","N_TOTAL_CLIPS = 1_500_000\n","TOTAL_CLIPS_TAKEN = 10_000\n","CLIPS_IDX_START = 100\n","LENGTH = 4\n","CLIPS_IDX_END = CLIPS_IDX_START + LENGTH - 1\n","CLIPS_TAKEN_PER_BATCH = max(1, int(N_VIDEOS_PER_BATCH / N_TOTAL_VIDEOS * TOTAL_CLIPS_TAKEN))\n","\n","metafile_path = './metafiles/hdvg_0.json'\n","classifier_filename = 'xgboost_model.pkl'\n","api_key = os.getenv(\"GEMINI_API_KEY\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-06T15:41:56.497305Z","iopub.status.busy":"2024-03-06T15:41:56.497041Z","iopub.status.idle":"2024-03-06T15:41:56.822410Z","shell.execute_reply":"2024-03-06T15:41:56.821459Z","shell.execute_reply.started":"2024-03-06T15:41:56.497283Z"},"trusted":true},"outputs":[],"source":["# file: inference.py\n","\n","# read the config file\n","with open('config.yaml', 'r') as f:\n","        config = yaml.safe_load(f)\n","\n","# setting the device\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","print(f'using device: {device}')\n","\n","# Parse the arguments\n","n_videos_per_batch = config[\"n_videos_per_batch\"]\n","clip_idx_start = config[\"clip_idx_start\"]\n","clip_idx_end = config[\"clip_idx_end\"]\n","store_intermediate_json = config[\"store_intermediate_json\"]\n","clips_taken_per_batch = max(1, int(config[\"clips_taken_per_batch\"]))\n","print(f'CLIPS_TAKEN_PER_BATCH: {clips_taken_per_batch}')\n","\n","# reading the metafile data\n","with open(metafile_path, 'r') as f:\n","    data = json.load(f)\n","\n","# Load the model for filtering\n","model = get_model()\n","model.to(device)\n","\n","inference_output_dir = 'frame_score_results'\n","if not os.path.exists(inference_output_dir):\n","    os.makedirs(inference_output_dir)\n","\n","# Load the classifier model\n","classifier_model = pickle.load(open(classifier_filename, 'rb'))\n","\n","# Create an instance of the GeminiRecaptioning class\n","gemini_recaptioning = GeminiRecaptioning(api_key, data)\n","\n","# Load the model for evaluation\n","eval_model, preprocess, tokenizer = get_eval_model()\n","eval_model = eval_model.to(device)\n","\n","for i in range(clip_idx_start, clip_idx_end+1, n_videos_per_batch):\n","    j = i + n_videos_per_batch - 1\n","    print(f'Processing Video Index {i}-{j}...')\n","    logging.info(f'Processing Video Index {i}-{j}...')\n","\n","    starttime = time.time()\n","    # getting the dataset\n","    dataset = VideoDataset(data, i, j)\n","    \n","    # getting the filtering metrics\n","    res = get_metrics(dataset, model, device)\n","\n","    if store_intermediate_json:\n","        # save the result if specified\n","        with open(os.path.join(inference_output_dir, f'inference_result_{i}-{j}.json'), 'w') as f:\n","            json.dump(res, f)\n","        print(f\"Saved to json: inference_result_{i}-{j}.json\")\n","\n","    # filter the scenes\n","    filtered_scenes = filter_scenes(res, clips_taken_per_batch, classifier_model)\n","    filtered_scenes = [dataset[idx] for idx in filtered_scenes]\n","    for scene in filtered_scenes:\n","        frames_path = scene['frames_path']\n","        assert(os.path.exists(frames_path))\n","        print(f\"Recaptioning for {frames_path}\")\n","        \n","        # Initialize the recaption variable\n","        recaption = \"\"\n","\n","        # Try the operation three times max\n","        for _ in range(3):\n","            try:\n","                recaption = gemini_recaptioning.run(frames_path).strip()\n","                break\n","            except:\n","                continue\n","\n","        scene['recaption'] = recaption\n","\n","    # save information of selected into json\n","    json_info_selected = {\n","        'length': len(filtered_scenes),\n","        'scenes': filtered_scenes\n","    }\n","\n","    print(f'Total time: {(time.time() - starttime):.2f}s')\n","\n","    # select CLIPS_TAKEN_PER_BATCH random idx from the dataset\n","    random_scenes = select_random_scenes(dataset, clips_taken_per_batch)\n","    random_scenes = [dataset[idx] for idx in random_scenes]\n","    for scene in random_scenes:\n","        frames_path = scene['frames_path']\n","        assert(os.path.exists(frames_path))\n","        print(f\"Recaptioning for {frames_path}\")\n","        \n","        # Initialize the recaption variable\n","        recaption = \"\"\n","\n","        # Try the operation three times max\n","        for _ in range(3):\n","            try:\n","                recaption = gemini_recaptioning.run(frames_path).strip()\n","                break\n","            except:\n","                continue\n","\n","        scene['recaption'] = recaption\n","    \n","    json_info_random = {\n","        'length': len(random_scenes),\n","        'scenes': random_scenes\n","    }\n","\n","    if store_intermediate_json:\n","        json_filename = f'random_scenes_{i}-{j}.json'\n","        with open(os.path.join(inference_output_dir, json_filename), 'w') as f:\n","            json.dump(json_info_random, f)\n","        print(f\"Saved to json: {json_filename}\")\n","\n","    # Run the evaluation\n","    print(\"Running the evaluation...\")\n","\n","    total_caption_score, total_recaption_score = eval_same_dataset(json_info_selected[\"scenes\"], preprocess, eval_model, tokenizer, device, logging)\n","    print(f\"Total caption score: {total_caption_score}\")\n","    print(f\"Total recaption score: {total_recaption_score}\")\n","\n","    total_score_random, total_score_selected  = eval_different_dataset(json_info_random[\"scenes\"], json_info_selected[\"scenes\"], preprocess, eval_model, tokenizer, device, logging)\n","    print(f\"Total score random: {total_score_random}\")\n","    print(f\"Total score selected: {total_score_selected}\")\n","\n","    # Add logging\n","    logging.info(f\"Total caption score: {total_caption_score}\")\n","    logging.info(f\"Total recaption score: {total_recaption_score}\")\n","    logging.info(f\"Total score random: {total_score_random}\")\n","    logging.info(f\"Total score selected: {total_score_selected}\")\n","\n","    json_info_selected[\"total_caption_score\"] = total_caption_score\n","    json_info_selected[\"total_recaption_score\"] = total_recaption_score\n","    json_info_selected[\"total_score_random\"] = total_score_random\n","    json_info_selected[\"total_score_selected\"] = total_score_selected\n","\n","    # Save the result to a json file\n","    json_filename = f'{str(i).zfill(5)}-{str(j).zfill(5)}.json'\n","    with open(os.path.join(inference_output_dir, json_filename), 'w') as f:\n","        json.dump(json_info_selected, f)\n","    \n","    print(f\"Saved to json: {json_filename}\")"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":4550537,"sourceId":7776939,"sourceType":"datasetVersion"},{"datasetId":4550559,"sourceId":7776972,"sourceType":"datasetVersion"}],"dockerImageVersionId":30665,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.2"}},"nbformat":4,"nbformat_minor":4}
